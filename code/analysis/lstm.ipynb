{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "import numpy as np\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import keras.backend as kb\n",
    "from PIL import Image\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, roc_auc_score, roc_curve\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampleMaker_entry(sample, input_size, output_size):\n",
    "    n_slides = len(sample.index) - (output_size + input_size)+1\n",
    "    entrada = np.array([np.array(sample[entrada_var].iloc[i:i+input_size]) for i in range(0, n_slides, input_size)])\n",
    "    return entrada\n",
    "\n",
    "def sampleMaker_out(sample, input_size, output_size):\n",
    "    n_slides = len(sample.index) - (output_size + input_size)+1\n",
    "    saida = [sample[saida_var].iloc[i+input_size:i+input_size+output_size] for i in range(0, n_slides, input_size)]\n",
    "    return saida\n",
    "\n",
    "def splitter(data, group):\n",
    "    data = list(data.groupby(group))\n",
    "    data = [data[i][1] for i in range(len(data))]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = pd.read_csv('/home/pasoneto/Documents/github/doc_suomi/data/lstm/lstm.csv')\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "base['valence_cat'] = le.fit_transform(base['valence_cat'])\n",
    "base['energy_cat'] = le.fit_transform(base['energy_cat'])\n",
    "base['loudness_cat'] = le.fit_transform(base['loudness_cat'])\n",
    "base['tempo_cat'] = le.fit_transform(base['tempo_cat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = splitter(base, \"album_id\")\n",
    "for i in base:\n",
    "    i.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "treino = base[0:int(len(base)*0.8)]\n",
    "teste = base[len(treino):len(base)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "entrada_var   = ['danceability', 'energy', 'loudness_overall', 'mode_confidence','speechiness', 'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo_overall', 'duration_ms', 'time_signature_confidence', 'loudness_continuous', 'tempo_continuous', 'tempo_confidence', 'key_confidence', 'danceability_cum', 'energy_cum','loudness_overall_cum', 'speechiness_cum', 'acousticness_cum','instrumentalness_cum', 'liveness_cum', 'valence_cum','tempo_overall_cum', 'duration_ms_cum', 'time_signature_cum','loudness_continuous_cum', 'tempo_continuous_cum','tempo_confidence_cum', 'key_confidence_cum', 'mode_confidence_cum','time_signature_confidence_cum']\n",
    "saida_var     = ['tempo_cat'] #'energy_cat', 'loudness_cat', 'tempo_cat', 'album_id']\n",
    "\n",
    "entrada_treino = list(map(lambda x : sampleMaker_entry(x, 5, 1), treino))\n",
    "saida_treino   = list(map(lambda x : sampleMaker_out(x, 5, 1), treino))\n",
    "\n",
    "entrada_teste = list(map(lambda x : sampleMaker_entry(x, 5, 1), teste))\n",
    "saida_teste   = list(map(lambda x : sampleMaker_out(x, 5, 1), teste))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import itertools\n",
    "entrada_treino = np.array(list(itertools.chain.from_iterable(entrada_treino)))\n",
    "saida_treino   = np.array(list(itertools.chain.from_iterable(saida_treino)))\n",
    "\n",
    "entrada_teste = np.array(list(itertools.chain.from_iterable(entrada_teste)))\n",
    "saida_teste   = np.array(list(itertools.chain.from_iterable(saida_teste)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo modelo\n",
    "regressor = Sequential()\n",
    "\n",
    "regressor.add(LSTM(units = 60, return_sequences = True, activation = 'relu', input_shape = (entrada_treino.shape[1], 33)))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "regressor.add(LSTM(units = 120, activation = 'sigmoid'))\n",
    "regressor.add(Dropout(0.5))\n",
    "\n",
    "regressor.add(Dense(units = 240, activation = 'sigmoid'))\n",
    "\n",
    "regressor.add(Dense(units = 1, activation = 'sigmoid'))\n",
    "\n",
    "regressor.compile(optimizer = 'sgd', loss = \"binary_crossentropy\", \n",
    "                  metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fitando modelo\n",
    "regressor.fit(entrada_treino, saida_treino, epochs = 1500, batch_size = 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "previsores = list(itertools.chain.from_iterable(regressor.predict_classes(entrada_teste)))\n",
    "real = list(itertools.chain.from_iterable(list(itertools.chain.from_iterable(saida_teste))))\n",
    "probs = list(itertools.chain.from_iterable(regressor.predict_proba(entrada_teste)))\n",
    "\n",
    "print(\"Model: \", accuracy_score(previsores, real))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 greater\n",
    "# 1 smaller\n",
    "for i in range(len(previsores)):\n",
    "    for k in range(len(previsores[i])):\n",
    "        if previsores[i][k] == 0:\n",
    "            previsores[i][k] = np.mean(real[i])+np.random.uniform(0.08, 0.1)\n",
    "        if previsores[i][k] == 1:\n",
    "            previsores[i][k] = np.mean(real[i])-np.random.uniform(0.08, 0.1)\n",
    "    plt.plot(previsores[i], 'ro', color = 'red', label = 'Valencia prevista')\n",
    "    plt.plot(real[i], 'ro', color = 'blue', label = 'Valencia real')\n",
    "    plt.plot(previsores[i], color = 'red')\n",
    "    plt.plot(real[i], color = 'blue')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving model - Energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialize model to JSON\n",
    "model_json = regressor.to_json()\n",
    "with open(\"model_tempo.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "regressor.save_weights(\"model_tempo.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading model\n",
    "\n",
    "Loading model's weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load json and create model\n",
    "from keras.models import model_from_json\n",
    "json_file = open('model_energy.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model_energy.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate loaded model on test data\n",
    "loaded_model.compile(optimizer = 'adam', loss = 'binary_crossentropy', \n",
    "                  metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "previsores = list(itertools.chain.from_iterable(loaded_model.predict_classes(entrada_teste)))\n",
    "real = list(itertools.chain.from_iterable(list(itertools.chain.from_iterable(saida_teste))))\n",
    "\n",
    "print(\"Model: \", accuracy_score(previsores, real))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
